```

```{r eval=TRUE, echo=FALSE, message=FALSE, purl=FALSE}
options(width = 90)
Sys.setenv(TZ = "UTC")
library(datadr)
library(trelliscope)
library(cyberTools)
setwd("~/Documents/Code/vastChallenge")
cl <- makeCluster(8)
clc <- localDiskControl(cluster = cl)
```

## Raw Data ETL ##

### Text Data to R Objects ###

One of the more tedious parts of data analaysis can be getting the data into the proper format for analysis.  `datadr` aspires to provide as much functionality to make this process as painless as possible, but there will always be special situations that require unique solutions.

For analysis in `datadr`, we want to take the raw data and store it as native R objects.  This provides a great degree of flexibility in what type of data structures we can use, such as non-tabular data or special classes of R objects like time series or spatial objects.

Here, all of our input data is text.  Text files are used quite often for storing and sharing big data.  For example, often [Hive](https://hive.apache.org) tables are stored as text files.  `datadr` provides some helpful functions that make it easy to deal with reading in text data and storing it as R objects..  

In this section we will go through how to read each of the data sources in from text.  In each case, we read the data in in chunks.  These examples read the data into `datadr`'s "local disk" storage mode using a helper function `drRead.csv()`.  This method also works for reading in text data on HDFS.

### NetFlow Data ###

The NetFlow data is located here: `data/raw/nf-week2.csv`.  To get a feel for what it looks like, we'll read in the first few rows using R's built-in function `read.csv()`.

<div class="callout callout-danger"><strong>Note: </strong>A common paradigm when using datadr is to test code on a subset of the data prior to applying it to the entire data set.  We will see this frequently throughout this document.</div>

#### Looking at a subset

To read in and look at the first 10 rows:

```{r eval=TRUE, echo=TRUE}
# read in 10 rows of netflow data
nfHead <- read.csv("data/raw/nf/nf-chunk1.csv", nrows = 10, stringsAsFactors = FALSE)
```

Here's what the first few rows and some of the columns of this data look like:

```{r eval=TRUE, echo=TRUE}
nfHead[1:10,3:7]
```

Let's look at the structure of the object to see all the columns and their data types:

```{r eval=TRUE, echo=TRUE}
# look at structure of the data
str(nfHead)
```

This looks like it is almost in a suitable form for analysis.  However, there are two columns that correspond to time, and neither is in a handy R-native format.  Instead of having a column for `TimeSeconds` and `parsedDate`, let's create a new column `time` that is an R `POSIXct` object.

```{r eval=TRUE, echo=TRUE}
# make new date variable
nfHead$date <- as.POSIXct(nfHead$TimeSeconds, origin = "1970-01-01", tz = "UTC")
# remove old time variables
nfHead <- nfHead[,setdiff(names(nfHead), c("TimeSeconds", "parsedDate"))]
```

Let's now make this operation a function, so that when we read in new rows of the data, we can just pass it through the function to obtain the preferred format:

```{r eval=TRUE, echo=TRUE}
nfTransform <- function(x) {
   x$date <- as.POSIXct(x$TimeSeconds, origin = "1970-01-01", tz = "UTC")
   x[,setdiff(names(x), c("TimeSeconds", "parsedDate"))]
}
```

We will use this function later.

Now that we have figured out what we want to do with the data, we can read the whole thing in.  But first we need to talk a little bit about disk connections in `datadr`.

#### Local disk connections

We will be storing the data we read in as a `datadr` *local disk connection*.  A local disk connection is defined by the path where we would like the data to be stored.  This should be an empty directory, and can be a nonexistent directory.

Here, we would like to store our parsed netflow data in `data/nfRaw`.  We initialize this connection with a call to `localDiskConn()`:

```{r eval=TRUE, results='hide', echo=TRUE, message=FALSE}
# initiate a new connection where parsed netflow data will be stored
nfConn <- localDiskConn("data/nfRaw")
```

This will prompt for whether you want the directory to be created if it does not exist.  `nfConn` is now simply an R object that points to this location on disk:

```{r eval=TRUE, echo=TRUE}
# look at the connection
nfConn
```

We can either add data to this connection using `addData()` or we can pass it as the `output` argument to our csv reader, as we will do in the following section.

#### Reading it all in

It turns out that there is a handy function in `datadr` that is the analog to `read.csv`, called `drRead.csv`, which reads the data in in blocks.  It has the same calling interface as R's `read.csv` with additional arguments to specify where to store the output, how many rows to put in each block, and an optional transformation function to apply to each block prior to storing it.

We will read in the netflow csv file using the default number of rows per block (`50000`), apply our `nfTransform` function that adds the `time` variable, and save the output to our `nfConn` local disk connection:

```{r eval=FALSE, echo=TRUE, results='hide', message=FALSE}
# read in netflow data
nfRaw <- drRead.csv("data/raw/nf", output = nfConn, postTransFn = nfTransform, rowsPerBlock = 200000)
```

Be prepared - the ETL operations using local disk are the most time-consuming tasks in this tutorial.  On my machine, the above command takes about 10 minutes to execute.  We will see that subsequent operations applied to the divided, parsed data are much faster.

<div class="callout callout-danger"><strong>Note: </strong>The drRead.csv function for local disk reads the data in sequentially.  However, drRead.csv operates in parallel when using the Hadoop backend.  There are a couple of reasons for sequential operation in local disk mode.  One is that simultaneous reads from the same single disk will probably not be faster, and could actually have worse performance (this is one of the most compelling reasons to use a distributed file system comprised of many disks such as what Hadoop provides).  Another related reason is the difficulty of having multiple processes scanning to different locations in a single file.</div>

```{r eval=FALSE, echo=FALSE, purl=FALSE}
{
# if we want port to be categorical:
# tmp[,"firstSeenSrcPort"] <- as.character(tmp[,"firstSeenSrcPort"])
# tmp[,"firstSeenDestPort"] <- as.character(tmp[,"firstSeenDestPort"])

# try running it with Spark (Heap space error)
h <- nfHead[1]
sc <- sparkR.init("local")
ff <- textFile(sc, "data/raw/nf-week2.csv", minSplits = 500)
nfRawSp <- lapplyPartition(ff, function(lines) { nfParse(lines, h) })
}
```

#### Distributed data objects

Let's take a look at `nfRaw` to see what the object looks like:

```{r eval=TRUE, echo=FALSE, message=FALSE, purl=FALSE}
# since we will have already updated the attributes, set them back to NA for a minute
# (doing this instead of caching...)
{
nfRaw <- ddf(localDiskConn("data/nfRaw"))
tmpNfAttr <- getAttributes(nfRaw, c("keys", "totObjectSize", "splitSizeDistn", "bsvInfo", "nRow", "splitRowDistn", "summary"))
nfRaw <- setAttributes(nfRaw, list(keys = NA, totObjectSize = NA, splitSizeDistn = NA, bsvInfo = NA, nRow = NA, splitRowDistn = NA, summary = NA))
}
```

```{r eval=TRUE, echo=TRUE}
nfRaw
```

`nfRaw` is a *distributed data frame* (ddf), and we see several aspects about the data printed.  For example, we see that there are 466 subsets and that the size of the parsed data in native R format is much smaller (`totStorageSize` = 171.98 MB) than the input text data.  The other attributes will be updated in a moment.

The `nfRaw` object itself is simply a special R object that contains metadata and pointers to the actual data stored on disk.  For more background on ddf and related objects, see [here](http://hafen.github.io/datadr/index.html#distributed-data-objects) and [here](http://hafen.github.io/datadr/index.html#distributed-data-frames), and particularly for ddf objects on local disk, see [here](http://hafen.github.io/datadr/index.html#medium-disk--multicore).

In any subsequent R session, we can "reload" this data object with the following:

```{r eval=TRUE, echo=TRUE, results='hide', message=FALSE, purl=FALSE}
nfRaw <- ddf(localDiskConn("data/nfRaw"))
```

Earlier we saw in the printout of `nfRaw` that it has many attibutes that have not yet been determined.  We can fix this by calling `updateAttributes()`:

```{r eval=FALSE, results='hide', echo=TRUE}
nfRaw <- updateAttributes(nfRaw, control = clc)
```

Here, through the `control` parameter, we specified that our local "cluster" we initialized at the beginning of our session should be used for the computation.  The update job takes about 30 seconds on my machine with 8 cores.

<div class="callout callout-danger"><strong>Note: </strong>This and most all other `datadr` methods can operate in a parallel fashion, where the configuration parameters for the parallel environment are specified through a <code>control</code> argument.</div>

Now we can see more meaningful information about our data:

```{r eval=TRUE, echo=FALSE, message=FALSE, purl=FALSE}
# set attributes back as if we updated nfRaw
nfRaw <- setAttributes(nfRaw, c(tmpNfAttr$ddo, tmpNfAttr$ddf))
```

```{r eval=TRUE, echo=TRUE}
nfRaw
```

We now see that there are nearly 70 million rows of data, and we are supplied, among other things, with summary statistics for the variables in the ddf which we will see in the next section.

```{r eval=FALSE, echo=FALSE, purl=FALSE}
{
system.time(nfRaw <- updateAttributes(nfRaw, control = clc))
# cores    user  system elapsed 
#     1 108.558  38.727 147.076
#     4   1.376   0.261  42.708
#     8   2.239   0.468  32.220

# just a note:
# on average it takes about 0.26 seconds to read and write one file (0.06 read, 0.2 write)
# there are 466 files - that's 121 seconds

a <- nfRaw[[1]]
aConn <- getAttribute(nfRaw, "conn")
fileSize <- file.info(file.path(aConn$loc, aConn$fileHashFn(list(a[[1]]), aConn)))$size
objSize <- object.size(a)
objSize / fileSize
# object size in memory is 12.5x bigger than on disk
}
```

#### Reading the data to HDFS

Before moving on it is worth noting how this data would be read in using Hadoop/HDFS as the backend.  The steps are identical except for the fact that we must first put the data on HDFS and then create an HDFS connection instead of a local disk connection.

To copy the data to HDFS:

```{r eval=FALSE, echo=TRUE}
library(Rhipe)
rhinit()

# create directory on HDFS for csv file
rhmkdir("/tmp/vast/raw")
# copy netflow csv from local disk to /tmp/vast/raw on HDFS
rhput("data/raw/nf-week2.csv", "/tmp/vast/raw")
```

Now to read the data in as a distributed data frame:

```{r eval=FALSE, echo=TRUE}
nfRaw <- drRead.csv(hdfsConn("tmp/vast/raw/nf-week2.csv", type = "text"), 
   output = hdfsConn("/tmp/vast/nfRaw"),
   postTransFn = nfTransform)
```


### IPS Data ###

We follow a similar approach for the IPS data.

```{r eval=TRUE, echo=TRUE}
# take a look at the data
ipsHead <- read.csv("data/raw/IPS-syslog-week2.csv", nrow = 10, stringsAsFactors = FALSE)
str(ipsHead)
```

Here, we have a different date/time input to deal with.  The 
Actually, it turns out that the `lubridate` package has a much faster implementation of `strptime`, called `fast_strptime`.  To use it, we will first replace `"Apr"` with `"04"` in the date/time string, and then call `fast_strptime` to convert the variable.

```{r eval=TRUE, echo=TRUE}
ipsHead$dateTime <- gsub("Apr", "04", ipsHead$dateTime)
ipsHead$dateTime <- fast_strptime(ipsHead$dateTime, format = "%d/%m/%Y %H:%M:%S", tz = "UTC")
```

Now we can build this into the transformation function with the additional step of renaming a couple of the columns of data:

```{r eval=FALSE, echo=TRUE}
# transformation to handle time variable
ipsTransform <- function(x) {
   require(lubridate)
   x$dateTime <- gsub("Apr", "04", x$dateTime)
   x$dateTime <- fast_strptime(x$dateTime, format = "%d/%m/%Y %H:%M:%S", tz = "UTC")
   names(x)[c(1, 6)] <- c("time", "srcIp")
   x
}

# read the data in
ipsRaw <- drRead.csv("data/raw/IPS-syslog-week2.csv",
   output = localDiskConn("data/ipsRaw"),
   postTransFn = ipsTransform)
```

As with the NetFlow data, we can call `updateAttributes()`:

```{r eval=TRUE, echo=FALSE, message=FALSE, purl=FALSE}
ipsRaw <- ddf(localDiskConn("data/ipsRaw"))
```

```{r eval=FALSE, results='hide', echo=TRUE}
ipsRaw <- updateAttributes(ipsRaw, control = clc)
```

```{r eval=TRUE, echo=TRUE}
ipsRaw
```

### Big Brother Data ###

The "big brother" data is handled similarly:

```{r eval=TRUE, echo=TRUE}
# look at first few rows
bbHead <- read.csv("data/raw/bb-week2.csv", nrows = 10, stringsAsFactors = FALSE)
str(bbHead)
```

There is one column that is very large in this data.  We have a similar task as before of parsing the time variale and removing some columns:

```{r eval=FALSE, echo=TRUE, message=FALSE}
# transformation to handle time parsing
bbTransform <- function(x) {
   x$time <- as.POSIXct(x$parsedDate, tz = "UTC")
   x[,setdiff(names(x), c("currenttime", "parsedDate"))]
}

bbRaw <- drRead.csv("data/raw/bb-week2.csv", 
   output = localDiskConn("data/bbRaw"), 
   postTransFn = bbTransform,
   autoColClasses = FALSE)
bbRaw <- updateAttributes(bbRaw, control = clc)
```

```{r eval=TRUE, echo=FALSE, message=FALSE, purl=FALSE}
bbRaw <- ddf(localDiskConn("data/bbRaw"))
```


```{r bbRawPrint, eval=TRUE, echo=TRUE}
bbRaw
```

```{r eval=FALSE, echo=FALSE, purl=FALSE}
{
clc2 <- clc
clc2$map_map_buff_size_bytes <- 10485760*10
clc2$map_reduce_buff_size_bytes <- 10485760*10
clc2$map_temp_buff_size_bytes <- 10485760*10
}
```

```{r eval=TRUE, echo=FALSE, purl=FALSE}
stopCluster(cl)
```
